from snakemake.utils import min_version
min_version("6.9.1")

# This pipeline takes a list of species names
# as input and downloads the corresponding genome
# assemblies where available. It then creates
# a multi-species fasta file and a table linking
# the taxon ids, genbank accession number and 
# species for all the sequences in the fasta file.
# Note that sequences less than 2000bp are excluded.

# Note that the resource usage for building indexes
# should be adjusted in the cluster.yml file 
# depending on the fasta file size. The script
# has been updated so that species ids are converted
# to species level taxids before searching for assemblies.


# Run as:
# snakemake -c 1 -s Snakefile_make_assm_db_all_verts --use-conda
#

# Make rulegraph with:
# snakemake -c 1 -s Snakefile_make_assm_db_all_verts --dag | dot -Tsvg > DAG_make_assm_db.svg

# List of species to download (one per line)
SPECIES ="all_verts_list_whuman_v2.txt"		

rule all:
	input:
		"all_verts_whuman.1.bt2l",
		"taxon_table.csv",
		"to_include.bed",
		"clean_done.txt"

# Download genbank assembly summary table
rule genbank_table:
	output:
		"gbk_summary/assembly_summary_genbank.txt"
	shell:
		"""
		wget https://ftp.ncbi.nlm.nih.gov/genomes/genbank/assembly_summary_genbank.txt
		mv assembly_summary_genbank.txt {output[0]}
		"""
# Convert species names to taxids
rule convert:
	input:
		SPECIES,
	output:
		"input_species_taxids.txt"
	conda:
		"envs/ete3.yml"
	shell:
		"""
		python scripts/convert_names2taxids.py {input[0]} {output[0]}
		"""	

# Make a summary table for target
# genomes
rule target_species:
	input:
		"input_species_taxids.txt",
		"gbk_summary/assembly_summary_genbank.txt"
	output:
		"assembly_summary_all_verts_whuman.txt"
	shell:
		"""
		bash scripts/make_summary_table.sh {input[0]} {input[1]} {output[0]}
		"""

# Make a script with paths to download genomes
# with wget. Use '-nc' so no download if the file
# exists.
rule make_ftp_script:
	input:
		"assembly_summary_all_verts_whuman.txt",
	output:
		temp("ftp_paths.txt"),
		"run_ftp_downloads.sh"
	shell:
		"""
		cut -f20 {input[0]} > {output[0]}
		awk 'BEGIN{{FS=OFS="/";filesuffix="genomic.fna.gz"}}\
			{{ftpdir=$0;asm=$10;file=asm"_"filesuffix;dir=$10;print \
			"wget -nc "ftpdir,file" -P "asm"/"}}'\
			{output[0]} > {output[1]}
		"""

# Download genomes
rule download_genomes:
	input:
		"run_ftp_downloads.sh"
	output:
		"download_log.txt"
	shell:
		"""
		bash {input[0]} 2> {output}
		"""

# Combine genomes
rule cat_genomes:
	input:
		"download_log.txt"
	output:
		temp("all_contigs.fasta")
	params:
		temp_out = "temp.fasta.gz"
	shell:
		"""
		cat GCA_*/*.gz > {params.temp_out}
		gunzip {params.temp_out}
		mv temp.fasta {output[0]}
		"""

# Remove contigs less than 2000bp
rule filt_short:
	input:
		"all_contigs.fasta"
	output:
		"all_verts_whuman.fasta"
	conda:
		"envs/seqkit.yml"
	shell:
		"""
		seqkit seq -m2000 {input[0]} -o {output[0]}
		"""


rule get_headers:
	input:
		"all_verts_whuman.fasta"
	output:
		"seq_ids.txt"
	shell:
		"""
		grep '>' {input[0]} | sed 's/>//' >> {output[0]}
		"""

# Note - Use fat node and all cores on Snowy
rule bowtie_idx:
	input:
		"all_verts_whuman.fasta"
	output:
		"all_verts_whuman.1.bt2l",
		"all_verts_whuman.2.bt2l",
		"all_verts_whuman.3.bt2l",
		"all_verts_whuman.4.bt2l",
		"all_verts_whuman.rev.1.bt2l",
		"all_verts_whuman.rev.2.bt2l"
	params:
		idx_base = "all_verts_whuman"
	threads: 16
	conda:
		"envs/bowtie2.yml"
	shell:
		"""
		bowtie2-build --large-index --threads {threads} {input[0]} {params.idx_base}
		"""

# Make a table with contig,species,taxid
rule make_taxon_table:
	input:
		"assembly_summary_all_verts_whuman.txt",
		"seq_ids.txt"
	output:
		"taxon_table.csv"
	conda:
		"envs/python.yml"
	shell:
		"""
		python scripts/make_id_table_with_taxid_v2.py {input[0]} {input[1]} {output[0]}
		"""

# Make bedfile that excludes Homo sapiens and
# Clupea harengus contigs

rule make_bedfile:
	input:
		"all_verts_whuman.fasta"
	output:
		"to_include.bed"
	conda:
		"envs/python.yml"
	shell:
		"""
		python scripts/make_bedfile_no_clupea_human.py {input[0]} {output[0]}
		"""


# Cleanup fastas
rule clean:
	input:
		"seq_ids.txt"
	output:
		"clean_done.txt"
	shell:
		"""
		rm -r GCA_*/
		head -n 0 {input[0]} >> {output[0]} 
		"""
